{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYhkf18ScJNWDmSgBSc9+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cur10usityDrives/Latent-Dirichlet-Allocation/blob/main/nlp_with_lda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHjtDSKwgNlu",
        "outputId": "35e095bd-81fb-4717-a8fe-18f3390630c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.116*\"topic\" + 0.093*\"modeling\" + 0.046*\"document\" + 0.046*\"used\" + 0.032*\"hidden\" + 0.032*\"machine\" + 0.032*\"learning\" + 0.032*\"collection\" + 0.032*\"discover\" + 0.032*\"technique\"\n",
            "Topic 1: 0.070*\"language\" + 0.042*\"using\" + 0.042*\"focused\" + 0.042*\"nlp\" + 0.042*\"human\" + 0.042*\"computer\" + 0.042*\"making\" + 0.042*\"study\" + 0.042*\"sense\" + 0.042*\"natural\"\n",
            "Coherence Score: 0.26095153549246014\n"
          ]
        }
      ],
      "source": [
        "# Install Gensim if you haven't already\n",
        "# !pip install gensim\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import download\n",
        "import string\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Topic modeling is an unsupervised machine learning technique used to discover hidden topics in a collection of documents.\",\n",
        "    \"Latent Dirichlet Allocation (LDA) is a popular topic modeling algorithm.\",\n",
        "    \"Gensim is a Python library for topic modeling.\",\n",
        "    \"Natural Language Processing (NLP) is a field of study focused on making sense of human language using computers.\",\n",
        "    \"Topic modeling can be used for clustering similar documents or for text summarization.\"\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "processed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "for topic_id, topic in lda_model.print_topics():\n",
        "    print(f\"Topic {topic_id}: {topic}\")\n",
        "\n",
        "# Compute coherence score\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15068e9c-ee1f-42d8-b0c6-3c96e3d081f8",
        "id": "eWeAENVIl-8o"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.062*\"affecting\" + 0.062*\"global\" + 0.037*\"defining\" + 0.037*\"issue\" + 0.037*\"change\" + 0.037*\"pattern\" + 0.037*\"investment\" + 0.037*\"time\" + 0.037*\"international\" + 0.037*\"trade\"\n",
            "Topic 1: 0.033*\"computing\" + 0.033*\"data\" + 0.033*\"processing\" + 0.033*\"capability\" + 0.033*\"could\" + 0.033*\"revolutionize\" + 0.033*\"speeding\" + 0.033*\"quantum\" + 0.033*\"significantly\" + 0.033*\"problem-solving\"\n",
            "Coherence Score: 0.2983492322315788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install Gensim if you haven't already\n",
        "# !pip install gensim\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import download\n",
        "import string\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence and robotics are leading the next wave of digital transformation.\",\n",
        "    \"Global markets are increasingly volatile, affecting international trade and investment strategies.\",\n",
        "    \"Advancements in biotechnology are making personalized medicine more accessible than ever.\",\n",
        "    \"Climate change is the defining issue of our time, affecting global weather patterns and ecosystems.\",\n",
        "    \"Quantum computing could revolutionize data processing by significantly speeding up problem-solving capabilities.\"\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "processed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "# Create dictionary and corpus\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "for topic_id, topic in lda_model.print_topics():\n",
        "    print(f\"Topic {topic_id}: {topic}\")\n",
        "\n",
        "# Compute coherence score\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")"
      ]
    }
  ]
}